@article{Kira1992,
abstract = {For real-world concept learning problems, feature selection is important to speed up learning and to improve concept quality. We review and analyze past approaches to feature selection and note their strengths and weaknesses. We then introduce and theoretically examine a new algorithm Relief which selects relevant features using a statistical method. Relief does not depend on heuristics, is accurate even if features interact, and is noise-tolerant. It requires only linear time in the number of given features and the number of training instances, regardless of the target concept complexity. The algorithm also has certain limitations such as non- optimal feature set size. Ways to overcome the limitations are suggested. We also report the test results of comparison between Relief and other feature selection algorithms. The empirical results support the theoretical analysis, suggesting a practical approach to feature selection},
author = {Kira, Kenji and Rendell, Larry A.},
doi = {10.1016/B978-1-55860-247-2.50037-1},
journal = {Machine Learning Proceedings 1992},
mendeley-groups = {pudu},
month = {jan},
pages = {249--256},
publisher = {Morgan Kaufmann},
title = {{A Practical Approach to Feature Selection}},
year = {1992}
}

@article{Kononenko1994,
abstract = {In the context of machine learning from examples this paper deals with the problem of estimating the quality of attributes with and without dependencies among them. Kira and Rendell (1992a,b) developed an algorithm called RELIEF, which was shown to be very efficient in estimating attributes. Original RELIEF can deal with discrete and continuous attributes and is limited to only two-class problems. In this paper RELIEF is analysed and extended to deal with noisy, incomplete, and multi-class data sets. The extensions are verified on various artificial and one well known real-world problem.},
author = {Kononenko, Igor},
doi = {10.1007/3-540-57868-4_57},
isbn = {9783540578680},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
mendeley-groups = {pudu},
pages = {171--182},
publisher = {Springer Verlag},
title = {{Estimating attributes: Analysis and extensions of RELIEF}},
url = {https://link.springer.com/chapter/10.1007/3-540-57868-4_57},
volume = {784 LNCS},
year = {1994}
}

@article{Petch2022,
abstract = {Many clinicians remain wary of machine learning because of longstanding concerns about “black box” models. “Black box” is shorthand for models that are sufficiently complex that they are not straightforwardly interpretable to humans. Lack of interpretability in predictive models can undermine trust in those models, especially in health care, in which so many decisions are— literally—life and death issues. There has been a recent explosion of research in the field of explainable machine learning aimed at addressing these concerns. The promise of explainable machine learning is considerable, but it is important for cardiologists who may encounter these techniques in clinical decision-support tools or novel research papers to have critical understanding of both their strengths and their limitations. This paper reviews key concepts and techniques in the field of explainable machine learning as they apply to cardiology. Key concepts reviewed include interpretability vs explainability and global vs local explanations. Techniques demonstrated include permutation importance, surrogate decision trees, local interpretable model-agnostic explanations, and partial dependence plots. We discuss several limitations with explainability techniques, focusing on the how the nature of explanations as approximations may omit important information about how black-box models work and why they make certain predictions. We conclude by proposing a rule of thumb about when it is appropriate to use black- box models with explanations rather than interpretable models.},
author = {Petch, Jeremy and Di, Shuang and Nelson, Walter},
doi = {10.1016/J.CJCA.2021.09.004},
issn = {0828-282X},
journal = {Canadian Journal of Cardiology},
mendeley-groups = {pudu},
month = {feb},
number = {2},
pages = {204--213},
pmid = {34534619},
publisher = {Elsevier},
title = {{Opening the Black Box: The Promise and Limitations of Explainable Machine Learning
  in Cardiology}},
volume = {38},
year = {2022}
}

@article{Ribeiro2016,
abstract = {Despite widespread adoption, machine learning models remain mostly black
boxes. Understanding the reasons behind predictions is, however, quite
important in assessing trust, which is fundamental if one plans to take action
based on a prediction, or when choosing whether to deploy a new model. Such
understanding also provides insights into the model, which can be used to
transform an untrustworthy model or prediction into a trustworthy one. In this
work, we propose LIME, a novel explanation technique that explains the
predictions of any classifier in an interpretable and faithful manner, by
learning an interpretable model locally around the prediction. We also propose
a method to explain models by presenting representative individual predictions
and their explanations in a non-redundant way, framing the task as a submodular
optimization problem. We demonstrate the flexibility of these methods by
explaining different models for text (e.g. random forests) and image
classification (e.g. neural networks). We show the utility of explanations via
novel experiments, both simulated and with human subjects, on various scenarios
that require trust: deciding if one should trust a prediction, choosing between
models, improving an untrustworthy classifier, and identifying why a classifier
should not be trusted.},
archivePrefix = {arXiv},
arxivId = {1602.04938},
author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
doi = {10.48550/arxiv.1602.04938},
eprint = {1602.04938},
isbn = {9781450342322},
journal = {NAACL-HLT 2016 - 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Proceedings of the Demonstrations Session},
mendeley-groups = {pudu},
month = {feb},
pages = {97--101},
publisher = {Association for Computational Linguistics (ACL)},
title = {{"Why Should I Trust You?": Explaining the Predictions of Any Classifier}},
url = {https://arxiv.org/abs/1602.04938v3},
year = {2016}
}

@article{Begley2020,
abstract = {As the decisions made or influenced by machine learning models increasingly
impact our lives, it is crucial to detect, understand, and mitigate unfairness.
But even simply determining what "unfairness" should mean in a given context is
non-trivial: there are many competing definitions, and choosing between them
often requires a deep understanding of the underlying task. It is thus tempting
to use model explainability to gain insights into model fairness, however
existing explainability tools do not reliably indicate whether a model is
indeed fair. In this work we present a new approach to explaining fairness in
machine learning, based on the Shapley value paradigm. Our fairness
explanations attribute a model's overall unfairness to individual input
features, even in cases where the model does not operate on sensitive
attributes directly. Moreover, motivated by the linearity of Shapley
explainability, we propose a meta algorithm for applying existing training-time
fairness interventions, wherein one trains a perturbation to the original
model, rather than a new model entirely. By explaining the original model, the
perturbation, and the fair-corrected model, we gain insight into the
accuracy-fairness trade-off that is being made by the intervention. We further
show that this meta algorithm enjoys both flexibility and stability benefits
with no loss in performance.},
archivePrefix = {arXiv},
arxivId = {2010.07389},
author = {Begley, Tom and Schwedes, Tobias and Frye, Christopher and Feige, Ilya},
doi = {10.48550/arxiv.2010.07389},
eprint = {2010.07389},
mendeley-groups = {pudu},
month = {oct},
title = {{Explainability for fair machine learning}},
url = {https://arxiv.org/abs/2010.07389v1},
year = {2020}
}

@article{Burkart2021,
abstract = {Predictions obtained by, e.g., artificial neural networks have a high accuracy
but humans often perceive the models as black boxes. Insights about the decision making are 
mostly opaque for humans. Particularly understanding the decision making in highly sensitive
areas such as healthcare or finance, is of paramount importance. The decision-making behind
the black boxes requires it to be more transparent, accountable, and understandable for
humans. This survey paper provides essential definitions, an overview of the different 
principles and methodologies of explainable Supervised Machine Learning (SML). We conduct
a state-of-the-art survey that reviews past and recent explainable SML approaches and 
classifies them according to the introduced definitions. Finally, we illustrate principles
by means of an explanatory case study and discuss important future directions.},
archivePrefix = {arXiv},
arxivId = {2011.07876},
author = {Burkart, Nadia and Huber, Marco F.},
doi = {10.1613/JAIR.1.12228},
eprint = {2011.07876},
issn = {1076-9757},
journal = {Journal of Artificial Intelligence Research},
keywords = {knowledge discovery,machine learning,neural networks,rule learning},
mendeley-groups = {pudu},
month = {jan},
pages = {245--317},
publisher = {AI Access Foundation},
title = {{A Survey on the Explainability of Supervised Machine Learning}},
url = {https://www.jair.org/index.php/jair/article/view/12228},
volume = {70},
year = {2021}
}

@article{Roscher2020,
abstract = {Machine learning methods have been remarkably successful for a wide range of
application areas in the extraction of essential information from data. An exciting and
relatively recent development is the uptake of machine learning in the natural sciences,
where the major goal is to obtain novel scientific insights and discoveries from observational
or simulated data. A prerequisite for obtaining a scientific outcome is domain knowledge, which
is needed to gain explainability, but also to enhance scientific consistency. In this article,
we review explainable machine learning in view of applications in the natural sciences and
discuss three core elements that we identified as relevant in this context: transparency,
interpretability, and explainability. With respect to these core elements, we provide a survey
of recent scientific works that incorporate machine learning and the way that explainable
machine learning is used in combination with domain knowledge from the application areas.},
archivePrefix = {arXiv},
arxivId = {1905.08883},
author = {Roscher, Ribana and Bohn, Bastian and Duarte, Marco F. and Garcke, Jochen},
doi = {10.1109/ACCESS.2020.2976199},
eprint = {1905.08883},
issn = {21693536},
journal = {IEEE Access},
keywords = {Explainable machine learning,informed machine learning,interpretability,scientific consistency,transparency},
mendeley-groups = {pudu},
pages = {42200--42216},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{Explainable Machine Learning for Scientific Insights and Discoveries}},
volume = {8},
year = {2020}
}

@article{Bhatt2020,
abstract = {Explainable machine learning offers the potential to provide stake-holders with
insights into model behavior by using various methods such as feature importance scores,
counterfactual explanations, or influential training data. Yet there is little understanding of
how organizations use these methods in practice. This study explores how organizations view and
use explainability for stakeholder consumption. We find that, currently, the majority of
deployments are not for end users affected by the model but rather for machine learning
engineers, who use explainability to debug the model itself. There is thus a gap between
explainability in practice and the goal of transparency, since explanations primarily serve
internal stakeholders rather than external ones. Our study synthesizes the limitations of
current explainability techniques that hamper their use for end users. To facilitate end user
interaction, we develop a framework for establishing clear goals for explainability. We end by
discussing concerns raised regarding explainability.},
author = {Bhatt, Umang and Xiang, Alice and Sharma, Shubham and Weller, Adrian and Taly, Ankur and Jia, Yunhan and Ghosh, Joydeep and Puri, Ruchir and Moura, Jos{\'{e}} M F and Eckersley, Peter},
doi = {10.1145/3351095.3375624},
isbn = {9781450369367},
keywords = {deployed systems,explainability,machine learning,qualitative study,transparency},
mendeley-groups = {pudu},
title = {{Explainable Machine Learning in Deployment}},
url = {https://doi.org/10.1145/3351095.3375624},
year = {2020}
}

@article{Belle2021,
abstract = {Artificial intelligence (AI) provides many opportunities to improve private and
public life. Discovering patterns and structures in large troves of data in an automated manner
is a core component of data science, and currently drives applications in diverse areas such as
computational biology, law and finance. However, such a highly positive impact is coupled with
a significant challenge: how do we understand the decisions suggested by these systems in order
that we can trust them? In this report, we focus specifically on data-driven methods—machine
learning (ML) and pattern recognition models in particular—so as to survey and distill the
results and observations from the literature. The purpose of this report can be especially
appreciated by noting that ML models are increasingly deployed in a wide range of businesses.
However, with the increasing prevalence and complexity of methods, business stakeholders in the
very least have a growing number of concerns about the drawbacks of models, data-specific
biases, and so on. Analogously, data science practitioners are often not aware about approaches
emerging from the academic literature or may struggle to appreciate the differences between
different methods, so end up using industry standards such as SHAP. Here, we have undertaken a
survey to help industry practitioners (but also data scientists more broadly) understand the
field of explainable machine learning better and apply the right tools. Our latter sections
build a narrative around a putative data scientist, and discuss how she might go about
explaining her models by asking the right questions. From an organization viewpoint, after
motivating the area broadly, we discuss the main developments, including the principles that
allow us to study transparent models vs. opaque models, as well as model-specific or
model-agnostic post-hoc explainability approaches. We also briefly reflect on deep learning
models, and conclude with a discussion about future research directions.},
archivePrefix = {arXiv},
arxivId = {2009.11698},
author = {Belle, Vaishak and Papantonis, Ioannis},
doi = {10.3389/FDATA.2021.688969},
eprint = {2009.11698},
issn = {2624909X},
journal = {Frontiers in Big Data},
keywords = {black-box models,explainable AI,machine learning,survey,transparent models},
mendeley-groups = {pudu},
month = {jul},
pages = {39},
pmid = {34278297},
publisher = {Frontiers Media S.A.},
title = {{Principles and Practice of Explainable Machine Learning}},
volume = {4},
year = {2021}
}

@article{Lundberg2017,
abstract = {Understanding why a model makes a certain prediction can be as crucial as the
prediction's accuracy in many applications. However, the highest accuracy for large modern
datasets is often achieved by complex models that even experts struggle to interpret, such as
ensemble or deep learning models, creating a tension between accuracy and interpretability. In
response, various methods have recently been proposed to help users interpret the predictions of
complex models, but it is often unclear how these methods are related and when one method is
preferable over another. To address this problem, we present a unified framework for
interpreting predictions, SHAP (SHapley Additive exPlanations). SHAP assigns each feature an
importance value for a particular prediction. Its novel components include: (1) the
identification of a new class of additive feature importance measures, and (2) theoretical
results showing there is a unique solution in this class with a set of desirable properties.
The new class unifies six existing methods, notable because several recent methods in the class
lack the proposed desirable properties. Based on insights from this unification, we present new
methods that show improved computational performance and/or better consistency with human
intuition than previous approaches.},
author = {Lundberg, Scott M and Allen, Paul G and Lee, Su-In},
journal = {Advances in Neural Information Processing Systems},
mendeley-groups = {pudu},
title = {{A Unified Approach to Interpreting Model Predictions}},
url = {https://github.com/slundberg/shap},
volume = {30},
year = {2017}
}

@misc{Selvaraju2017,
abstract = {We propose a technique for producing 'visual explana-tions' for decisions from a
large class of Convolutional Neu-ral Network (CNN)-based models, making them more transparent.
Our approach-Gradient-weighted Class Activation Mapping (Grad-CAM), uses the gradients of any
target concept (say logits for 'dog' or even a caption), flowing into the final convolutional
layer to produce a coarse localiza-tion map highlighting the important regions in the image for
predicting the concept. Unlike previous approaches, Grad-CAM is applicable to a wide variety of
CNN model-families: (1) CNNs with fully-connected layers (e.g. VGG), (2) CNNs used for
structured outputs (e.g. captioning), (3) CNNs used in tasks with multi-modal inputs (e.g.
visual question answering) or reinforcement learning, without architectural changes or
retraining. We combine Grad-CAM with existing fine-grained visualizations to create a high-resolution class-discriminative visualization, Guided Grad-CAM, and apply it to image classification, image captioning, and visual question answering (VQA) models, including ResNet-based architectures. In the context of image classification models , our visualizations (a) lend insights into failure modes of these models (showing that seemingly unreasonable predictions have reasonable explanations), (b) outperform previous methods on the ILSVRC-15 weakly-supervised local-ization task, (c) are more faithful to the underlying model, and (d) help achieve model generalization by identifying dataset bias. For image captioning and VQA, our visual-izations show even non-attention based models can localize inputs. Finally, we design and conduct human studies to measure if Grad-CAM explanations help users establish appropriate trust in predictions from deep networks and show that Grad-CAM helps untrained users successfully discern a 'stronger' deep network from a 'weaker' one even when both make identical predictions. Our code is available at https: //github.com/ramprs/grad-cam/ along with a demo on CloudCV [2] 1 and video at youtu.be/COjUB9Izk6E. * Work done at Virginia Tech. 1},
author = {Selvaraju, Ramprasaath R. and Cogswell, Michael and Das, Abhishek and Vedantam,
Ramakrishna and Parikh, Devi and Batra, Dhruv},
journal = {Proceedings of the IEEE International Conference on Computer Vision},
doi = {10.1109/iccv.2017.74},
mendeley-groups = {pudu},
pages = {618--626},
title = {{Grad-CAM: Visual Explanations From Deep Networks via Gradient-Based Localization}},
url = {http://gradcam.cloudcv.org},
year = {2017}
}


@techreport{Pedregosa2011,
abstract = {Scikit-learn is a Python module integrating a wide range of state-of-the-art
machine learning algorithms for medium-scale supervised and unsupervised problems. This package
focuses on bringing machine learning to non-specialists using a general-purpose high-level
language. Emphasis is put on ease of use, performance, documentation, and API consistency. It
has minimal dependencies and is distributed under the simplified BSD license, encouraging its
use in both academic and commercial settings. Source code, binaries, and documentation can be
downloaded from http://scikit-learn.sourceforge.net.},
author = {Pedregosa, Fabian and Varoquaux, Ga{\"{e}}l and Gramfort, Alexandre and Michel,
Vincent and Thirion, Bertrand and Grisel, Olivier and Blondel, Mathieu and Prettenhofer,
Peter and Weiss, Ron and Dubourg, Vincent and Vanderplas, Jake and Passos, Alexandre and
Cournapeau, David and Brucher, Matthieu and Perrot, Matthieu and Duchesnay, {\'{E}}douard},
booktitle = {Journal of Machine Learning Research},
keywords = {Python,model selection,supervised learning,unsupervised learning},
mendeley-groups = {SuperPaper},
pages = {2825--2830},
title = {{Scikit-learn: Machine Learning in Python}},
url = {http://scikit-learn.sourceforge.net.},
volume = {12},
year = {2011}
}

@article{chollet2018keras,
  title={Keras: The python deep learning library},
  author={Chollet, Fran{\c{c}}ois and others},
  journal={Astrophysics source code library},
  pages={ascl--1806},
  year={2018}
}


@misc{Marholm2022,
author = {Marholm, Sigvald},
doi = {10.5281/ZENODO.6344451},
mendeley-groups = {pudu},
month = {mar},
title = {{sigvaldm/localreg: Multivariate RBF output}},
url = {https://zenodo.org/record/6344451},
year = {2022}
}
