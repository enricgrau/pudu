@article{Goodacre2003,
abstract = {Whole organism or tissue profiling by vibrational spectroscopy produces vast amounts of seemingly unintelligible data. However, the characterisation of the biological system under scrutiny is generally possible only in combination with modern supervised machine learning techniques, such as artificial neural networks (ANNs). Nevertheless, the interpretation of the calibration models from ANNs is often very difficult, and the information in terms of which vibrational modes in the infrared or Raman spectra are important is not readily available. ANNs are often perceived as 'black box' approaches to modelling spectra, and to allow the deconvolution of complex hyperspectral data it is necessary to develop a system that itself produces 'rules' that are readily comprehensible. Evolutionary computation, and in particular genetic programming (GP), is an ideal method to achieve this. An example of how GP can be used for Fourier transform infrared (FT-IR) image analysis is presented, and is compared with images produced by principal components analysis (PCA), discriminant function analysis (DFA) and partial least squares (PLS) regression. {\textcopyright} 2003 Elsevier B.V. All rights reserved.},
author = {Goodacre, Royston},
doi = {10.1016/S0924-2031(03)00045-6},
issn = {0924-2031},
journal = {Vibrational Spectroscopy},
keywords = {Artificial neural networks,FT-IR,Genetic programming},
month = {aug},
number = {1},
pages = {33--45},
publisher = {Elsevier},
title = {{Explanatory analysis of spectroscopic data using machine learning of simple, interpretable rules}},
volume = {32},
year = {2003}
}


@article{Luo2022,
abstract = {Raman spectroscopy (RS) is a spectroscopic method which indirectly measures the vibrational states within samples. This information on vibrational states can be utilized as spectroscopic fingerprints of the sample, which, subsequently, can be used in a wide range of application scenarios to determine the chemical composition of the sample without altering it, or to predict a sample property, such as the disease state of patients. These two examples are only a small portion of the application scenarios, which range from biomedical diagnostics to material science questions. However, the Raman signal is weak and due to the label-free character of RS, the Raman data is untargeted. Therefore, the analysis of Raman spectra is challenging and machine learning based chemometric models are needed. As a subset of representation learning algorithms, deep learning (DL) has had great success in data science for the analysis of Raman spectra and photonic data in general. In this review, recent developments of DL algorithms for Raman spectroscopy and the current challenges in the application of these algorithms will be discussed.},
author = {Luo, Ruihao and Popp, Juergen and Bocklitz, Thomas},
doi = {10.3390/analytica3030020},
file = {:C\:/Users/Enric/Downloads/analytica-03-00020.pdf:pdf},
issn = {26734532},
journal = {Analytica},
keywords = {Raman spectroscopy,chemometrics,deep learning,machine learning},
number = {3},
pages = {287--301},
title = {{Deep Learning for Raman Spectroscopy: A Review}},
volume = {3},
year = {2022}
}


@article{Easton2020,
abstract = {XPS is widely used to identify and quantify the elements present at the surface of polymeric materials. The energy distribution of photoelectrons emitted from these elements contains information about their chemical state, potentially allowing the analyst to identify and quantify specific functional groups. These functional groups may originate from the synthesis and processing of the polymers, from postsynthetic modifications such as surface grafting, or indeed may be unrelated to the polymer (additives and contaminants). Extracting reliable and meaningful information from XPS data is not trivial and relies on careful and appropriate experimentation, including experimental design, sample preparation, data collection, data processing, and data interpretation. Here, the authors outline some of these challenges when performing XPS analysis of polymers and provide practical examples to follow. This guide will cover all relevant aspects over the course of a typical experiment, including tips and considerations when designing the experiment, sample preparation, charge neutralization, x-ray induced sample damage, depth profiling, data analysis and interpretation, and, finally, reporting of results. Many of these topics are more widely applicable to insulating organic materials, and the recommendations of this guide will help to ensure that data is collected and interpreted using current best practices.},
author = {Easton, Christopher D. and Kinnear, Calum and McArthur, Sally L. and Gengenbach, Thomas R.},
doi = {10.1116/1.5140587/247679},
issn = {0734-2101},
journal = {Journal of Vacuum Science & Technology A: Vacuum, Surfaces, and Films},
month = {mar},
number = {2},
publisher = {American Vacuum Society},
title = {{Practical guides for x-ray photoelectron spectroscopy: Analysis of polymers}},
url = {/avs/jva/article/38/2/023207/247679/Practical-guides-for-x-ray-photoelectron},
volume = {38},
year = {2020}
}


@article{Guc2016,
abstract = {A non-destructive Raman spectroscopy has been widely used as a complimentary method to X-ray diffraction characterization of Cu2ZnSnS4 (CZTS) thin films, yet our knowledge of the Raman active fundamental modes in this material is far from complete. Focusing on polarized Raman spectroscopy provides important information about the relationship between Raman modes and CZTS crystal structure. In this framework the zone–center optical phonons of CZTS, which is most usually examined in active layers of the CZTS based solar cells, are studied by polarized resonant and non-resonant Raman spectroscopy in the range from 60 to 500 cm−1 on an oriented single crystal. The phonon mode symmetry of 20 modes from the 27 possible vibrational modes of the kesterite structure is experimentally determined. From in-plane angular dependences of the phonon modes intensities Raman tensor elements are also derived. Whereas a strong intensity enhancement of the polar E and B symmetry modes is induced under resonance conditions, no mode intensity dependence on the incident and scattered light polarization configurations was found in these conditions. Finally, Lyddane-Sachs-Teller relations are applied to estimate the ratios of the static to high-frequency optic dielectric constants parallel and perpendicular to c-optical axis.},
author = {Guc, Maxim and Levcenko, Sergiu and Bodnar, Ivan V. and Izquierdo-Roca, Victor and Fontane, Xavier and Volkova, Larisa V. and Arushanov, Ernest and P{\'{e}}rez-Rodr{\'{i}}guez, Alejandro},
doi = {10.1038/srep19414},
issn = {2045-2322},
journal = {Scientific Reports 2016 6:1},
keywords = {Raman spectroscopy,Semiconductors,Solar cells},
month = {jan},
number = {1},
pages = {1--7},
publisher = {Nature Publishing Group},
title = {{Polarized Raman scattering study of kesterite type Cu2ZnSnS4 single crystals}},
url = {https://www.nature.com/articles/srep19414},
volume = {6},
year = {2016}
}



@article{Haruna2023,
abstract = {Infrared (IR) spectroscopy is a spectroscopic technique dealing with the interaction of IR light with molecules. Molecules absorb IR light that are characteristics of their structure and matches their vibrational frequency. The absorption, emission, and reflection of IR radiation are based on changes in vibrational or rotational energy states of molecules. The IR technique can analyze and identify gaseous, liquid, or solid samples of compounds. IR techniques have been used widely in corrosion studies and its control. In this chapter, we have highlighted the basic theory of IR spectroscopy, the different IR instrumentations, and techniques. Some selected studies reported in the literature on the application of IR technique to understand corrosion-related phenomena like corrosion inhibition, microbiologically induced corrosion, atmospheric corrosion, and coating and surface treatment are also presented.},
address = {New York},
author = {Haruna, K. and Obot, I. B. and Saleh, T. A.},
doi = {10.1201/9781003328513-9},
isbn = {9781003328513},
journal = {Corrosion Science},
month = {oct},
pages = {261--289},
publisher = {Apple Academic Press},
title = {{Infrared Spectroscopy in Corrosion Research}},
url = {https://www.taylorfrancis.com/chapters/edit/10.1201/9781003328513-9/infrared-spectroscopy-corrosion-research-haruna-obot-saleh},
year = {2023}
}


@article{Estefany2023,
abstract = {Atmosphere aerosols have significant impact on human health and the environment. Aerosol particles have a number of characteristics that influence their health and environmental effects, including their size, shape, and chemical composition. A great deal of difficulty is associated with quantifying and identifying atmospheric aerosols because these parameters are highly variable on a spatial and temporal scale. An important component of understanding aerosol fate is Raman Spectroscopy (RS), which is capable of resolving chemical compositions of individual particles. This review presented strategic techniques, especially RS methods for characterizing atmospheric aerosols. The nature and properties of atmospheric aerosols and their influence on environment and human health were briefly described. Analytical methodologies that offer insight into the chemistry and multidimensional properties of aerosols were discussed. In addition, perspectives for practical applications of atmospheric aerosols using RS are featured.},
author = {Estefany, Cede{\~{n}}o and Sun, Zhenli and Hong, Zijin and Du, Jingjing},
doi = {10.1016/J.ECOENV.2022.114405},
issn = {0147-6513},
journal = {Ecotoxicology and Environmental Safety},
keywords = {Atmospheric aerosols,Raman imaging,Raman spectroscopy,SERS,TERS},
month = {jan},
pages = {114405},
pmid = {36508807},
publisher = {Academic Press},
title = {{Raman spectroscopy for profiling physical and chemical properties of atmospheric aerosol particles: A review}},
volume = {249},
year = {2023}
}


@article{Bhatt2023,
abstract = {In countless fields of research and characterization of compounds, spectroscopes have been proven extremely beneficial. Spectroscopes have proven to be a promising approach in criminological examination and crime evidence analysis. Each spectroscopy has its own characteristic working and applications. Spectroscopic methods and forensics are thus, claimed to be having an unbroken bond. There are various traditional approaches applied by experts in analysis that are destructive and outdated and can be replaced with the modern and non-destructive spectroscopic investigation. This chapter summarizes the classification, instrumentation, and working of the spectroscopes with diagram together with spectroscopy and its importance in criminology. Additionally, the application of each and every spectroscopy in evidence analysis is highlighted. The importance of using the non-destructive and precise spectroscopic techniques in crime investigation and its future development and implications are also considered and discussed. New evolutions in the sector of spectroscopic approaches can open on to fully automated and in situ investigation of crime.},
author = {Bhatt, Payal V. and Rawtani, Deepak},
doi = {10.1002/9781119763406.CH8},
isbn = {9781119763406},
journal = {Modern Forensic Tools and Devices: Trends in Criminal Investigation},
keywords = {Spectroscopy,absorption,analysis techniques,chemiluminisense,criminal examination,emission,forensic science,non destructive methods,photoluminisense},
month = {jan},
pages = {149--197},
publisher = {John Wiley & Sons, Ltd},
title = {{Spectroscopic Analysis Techniques in Forensic Science}},
url = {https://onlinelibrary.wiley.com/doi/full/10.1002/9781119763406.ch8 https://onlinelibrary.wiley.com/doi/abs/10.1002/9781119763406.ch8 https://onlinelibrary.wiley.com/doi/10.1002/9781119763406.ch8},
year = {2023}
}


@article{Fischer2007,
abstract = {Tunneling spectroscopy has played a central role in the experimental verification of the microscopic theory of superconductivity in classical superconductors. Initial attempts to apply the same approach to high-temperature superconductors were hampered by various problems related to the complexity of these materials. The use of scanning tunneling microscopy and spectroscopy (STM and STS) on these compounds allowed the main difficulties to be overcome. This success motivated a rapidly growing scientific community to apply this technique to high-temperature superconductors. This paper reviews the experimental highlights obtained over the last decade. The crucial efforts to gain control over the technique and to obtain reproducible results are first recalled. Then a discussion on how the STM and STS techniques have contributed to the study of some of the most unusual and remarkable properties of high-temperature superconductors is presented: the unusually large gap values and the absence of scaling with the critical temperature, the pseudogap and its relation to superconductivity, the unprecedented small size of the vortex cores and its influence on vortex matter, the unexpected electronic properties of the vortex cores, and the combination of atomic resolution and spectroscopy leading to the observation of periodic local density of states modulations in the superconducting and pseudogap states and in the vortex cores. {\textcopyright} 2007 The American Physical Society.},
archivePrefix = {arXiv},
arxivId = {cond-mat/0610672},
author = {Fischer, {\O}ystein and Kugler, Martin and Maggio-Aprile, Ivan and Berthod, Christophe and Renner, Christoph},
doi = {10.1103/REVMODPHYS.79.353},
eprint = {0610672},
issn = {00346861},
journal = {Reviews of Modern Physics},
month = {mar},
number = {1},
pages = {353--419},
primaryClass = {cond-mat},
publisher = {American Physical Society},
title = {{Scanning tunneling spectroscopy of high-temperature superconductors}},
url = {https://journals.aps.org/rmp/abstract/10.1103/RevModPhys.79.353},
volume = {79},
year = {2007}
}



@article{Bellisola2012,
abstract = {Since the middle of 20<sup>th</sup> century infrared (IR) spectroscopy coupled to microscopy (IR microspectroscopy) has been recognized as a non destructive, label free, highly sensitive and specific analytical method with many potential useful applications in different fields of biomedical research and in particular cancer research and diagnosis. Although many technological improvements have been made to facilitate biomedical applications of this powerful analytical technique, it has not yet properly come into the scientific background of many potential end users. Therefore, to achieve those fundamental objectives an interdisciplinary approach is needed with basic scientists, spectroscopists, biologists and clinicians who must effectively communicate and understand each other's requirements and challenges. In this review we aim at illustrating some principles of Fourier transform (FT) Infrared (IR) vibrational spectroscopy and microscopy (microFT-IR) as a useful method to interrogate molecules in specimen by mid-IR radiation. Penetrating into basics of molecular vibrations might help us to understand whether, when and how complementary information obtained by microFT-IR could become useful in our research and/or diagnostic activities. MicroFT-IR techniques allowing to acquire information about the molecular composition and structure of a sample within a micrometric scale in a matter of seconds will be illustrated as well as some limitations will be discussed. How biochemical, structural, and dynamical information about the systems can be obtained by bench top microFT-IR instrumentation will be also presented together with some methods to treat and interpret IR spectral data and applicative examples. The mid-IR absorbance spectrum is one of the most information-rich and concise way to represent the whole "... omics" of a cell and, as such, fits all the characteristics for the development of a clinically useful biomarker.},
author = {Bellisola, Giuseppe and Sorio, Claudio},
issn = {21566976},
journal = {American Journal of Cancer Research},
keywords = {Cancer biomarker,Cancer diagnosis,Infrared microspectroscopy,Infrared radiation,Mid-infrared absorbance spectroscopy,Molecular vibrations,Pre-clinical drug screening,Synchrotron radiation,Unsupervised multivariate analysis,Vibrational spectroscopy},
number = {1},
pages = {1},
pmid = {22206042},
publisher = {e-Century Publishing Corporation},
title = {{Infrared spectroscopy and microscopy in cancer research and diagnosis}},
url = {/pmc/articles/PMC3236568/ /pmc/articles/PMC3236568/?report=abstract https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3236568/},
volume = {2},
year = {2012}
}


@article{Zhong2022,
abstract = {Machine learning models are increasingly used in materials studies because of their exceptional accuracy. However, the most accurate machine learning models are usually difficult to explain. Remedies to this problem lie in explainable artificial intelligence (XAI), an emerging research field that addresses the explainability of complicated machine learning models like deep neural networks (DNNs). This article attempts to provide an entry point to XAI for materials scientists. Concepts are defined to clarify what explain means in the context of materials science. Example works are reviewed to show how XAI helps materials science research. Challenges and opportunities are also discussed.},
author = {Zhong, Xiaoting and Gallagher, Brian and Liu, Shusen and Kailkhura, Bhavya and Hiszpanski, Anna and Han, T. Yong Jin},
doi = {10.1038/s41524-022-00884-7},
issn = {2057-3960},
journal = {npj Computational Materials 2022 8:1},
keywords = {Techniques and instrumentation,Theory and computation},
month = {sep},
number = {1},
pages = {1--19},
publisher = {Nature Publishing Group},
title = {{Explainable machine learning in materials science}},
url = {https://www.nature.com/articles/s41524-022-00884-7},
volume = {8},
year = {2022}
}



@article{Kira1992,
abstract = {For real-world concept learning problems, feature selection is important to speed up learning and to improve concept quality. We review and analyze past approaches to feature selection and note their strengths and weaknesses. We then introduce and theoretically examine a new algorithm Relief which selects relevant features using a statistical method. Relief does not depend on heuristics, is accurate even if features interact, and is noise-tolerant. It requires only linear time in the number of given features and the number of training instances, regardless of the target concept complexity. The algorithm also has certain limitations such as non- optimal feature set size. Ways to overcome the limitations are suggested. We also report the test results of comparison between Relief and other feature selection algorithms. The empirical results support the theoretical analysis, suggesting a practical approach to feature selection},
author = {Kira, Kenji and Rendell, Larry A.},
doi = {10.1016/B978-1-55860-247-2.50037-1},
journal = {Machine Learning Proceedings 1992},
mendeley-groups = {pudu},
month = {jan},
pages = {249--256},
publisher = {Morgan Kaufmann},
title = {{A Practical Approach to Feature Selection}},
year = {1992}
}

@article{Kononenko1994,
abstract = {In the context of machine learning from examples this paper deals with the problem of estimating the quality of attributes with and without dependencies among them. Kira and Rendell (1992a,b) developed an algorithm called RELIEF, which was shown to be very efficient in estimating attributes. Original RELIEF can deal with discrete and continuous attributes and is limited to only two-class problems. In this paper RELIEF is analysed and extended to deal with noisy, incomplete, and multi-class data sets. The extensions are verified on various artificial and one well known real-world problem.},
author = {Kononenko, Igor},
doi = {10.1007/3-540-57868-4_57},
isbn = {9783540578680},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
mendeley-groups = {pudu},
pages = {171--182},
publisher = {Springer Verlag},
title = {{Estimating attributes: Analysis and extensions of RELIEF}},
url = {https://link.springer.com/chapter/10.1007/3-540-57868-4_57},
volume = {784 LNCS},
year = {1994}
}

@article{Petch2022,
abstract = {Many clinicians remain wary of machine learning because of longstanding concerns about “black box” models. “Black box” is shorthand for models that are sufficiently complex that they are not straightforwardly interpretable to humans. Lack of interpretability in predictive models can undermine trust in those models, especially in health care, in which so many decisions are— literally—life and death issues. There has been a recent explosion of research in the field of explainable machine learning aimed at addressing these concerns. The promise of explainable machine learning is considerable, but it is important for cardiologists who may encounter these techniques in clinical decision-support tools or novel research papers to have critical understanding of both their strengths and their limitations. This paper reviews key concepts and techniques in the field of explainable machine learning as they apply to cardiology. Key concepts reviewed include interpretability vs explainability and global vs local explanations. Techniques demonstrated include permutation importance, surrogate decision trees, local interpretable model-agnostic explanations, and partial dependence plots. We discuss several limitations with explainability techniques, focusing on the how the nature of explanations as approximations may omit important information about how black-box models work and why they make certain predictions. We conclude by proposing a rule of thumb about when it is appropriate to use black- box models with explanations rather than interpretable models.},
author = {Petch, Jeremy and Di, Shuang and Nelson, Walter},
doi = {10.1016/J.CJCA.2021.09.004},
issn = {0828-282X},
journal = {Canadian Journal of Cardiology},
mendeley-groups = {pudu},
month = {feb},
number = {2},
pages = {204--213},
pmid = {34534619},
publisher = {Elsevier},
title = {{Opening the Black Box: The Promise and Limitations of Explainable Machine Learning
  in Cardiology}},
volume = {38},
year = {2022}
}

@article{Ribeiro2016,
abstract = {Despite widespread adoption, machine learning models remain mostly black
boxes. Understanding the reasons behind predictions is, however, quite
important in assessing trust, which is fundamental if one plans to take action
based on a prediction, or when choosing whether to deploy a new model. Such
understanding also provides insights into the model, which can be used to
transform an untrustworthy model or prediction into a trustworthy one. In this
work, we propose LIME, a novel explanation technique that explains the
predictions of any classifier in an interpretable and faithful manner, by
learning an interpretable model locally around the prediction. We also propose
a method to explain models by presenting representative individual predictions
and their explanations in a non-redundant way, framing the task as a submodular
optimization problem. We demonstrate the flexibility of these methods by
explaining different models for text (e.g. random forests) and image
classification (e.g. neural networks). We show the utility of explanations via
novel experiments, both simulated and with human subjects, on various scenarios
that require trust: deciding if one should trust a prediction, choosing between
models, improving an untrustworthy classifier, and identifying why a classifier
should not be trusted.},
archivePrefix = {arXiv},
arxivId = {1602.04938},
author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
doi = {10.48550/arxiv.1602.04938},
eprint = {1602.04938},
isbn = {9781450342322},
journal = {NAACL-HLT 2016 - 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Proceedings of the Demonstrations Session},
mendeley-groups = {pudu},
month = {feb},
pages = {97--101},
publisher = {Association for Computational Linguistics (ACL)},
title = {{"Why Should I Trust You?": Explaining the Predictions of Any Classifier}},
url = {https://arxiv.org/abs/1602.04938v3},
year = {2016}
}

@article{Begley2020,
abstract = {As the decisions made or influenced by machine learning models increasingly
impact our lives, it is crucial to detect, understand, and mitigate unfairness.
But even simply determining what "unfairness" should mean in a given context is
non-trivial: there are many competing definitions, and choosing between them
often requires a deep understanding of the underlying task. It is thus tempting
to use model explainability to gain insights into model fairness, however
existing explainability tools do not reliably indicate whether a model is
indeed fair. In this work we present a new approach to explaining fairness in
machine learning, based on the Shapley value paradigm. Our fairness
explanations attribute a model's overall unfairness to individual input
features, even in cases where the model does not operate on sensitive
attributes directly. Moreover, motivated by the linearity of Shapley
explainability, we propose a meta algorithm for applying existing training-time
fairness interventions, wherein one trains a perturbation to the original
model, rather than a new model entirely. By explaining the original model, the
perturbation, and the fair-corrected model, we gain insight into the
accuracy-fairness trade-off that is being made by the intervention. We further
show that this meta algorithm enjoys both flexibility and stability benefits
with no loss in performance.},
archivePrefix = {arXiv},
arxivId = {2010.07389},
author = {Begley, Tom and Schwedes, Tobias and Frye, Christopher and Feige, Ilya},
doi = {10.48550/arxiv.2010.07389},
eprint = {2010.07389},
mendeley-groups = {pudu},
month = {oct},
title = {{Explainability for fair machine learning}},
url = {https://arxiv.org/abs/2010.07389v1},
year = {2020}
}

@article{Burkart2021,
abstract = {Predictions obtained by, e.g., artificial neural networks have a high accuracy
but humans often perceive the models as black boxes. Insights about the decision making are 
mostly opaque for humans. Particularly understanding the decision making in highly sensitive
areas such as healthcare or finance, is of paramount importance. The decision-making behind
the black boxes requires it to be more transparent, accountable, and understandable for
humans. This survey paper provides essential definitions, an overview of the different 
principles and methodologies of explainable Supervised Machine Learning (SML). We conduct
a state-of-the-art survey that reviews past and recent explainable SML approaches and 
classifies them according to the introduced definitions. Finally, we illustrate principles
by means of an explanatory case study and discuss important future directions.},
archivePrefix = {arXiv},
arxivId = {2011.07876},
author = {Burkart, Nadia and Huber, Marco F.},
doi = {10.1613/JAIR.1.12228},
eprint = {2011.07876},
issn = {1076-9757},
journal = {Journal of Artificial Intelligence Research},
keywords = {knowledge discovery,machine learning,neural networks,rule learning},
mendeley-groups = {pudu},
month = {jan},
pages = {245--317},
publisher = {AI Access Foundation},
title = {{A Survey on the Explainability of Supervised Machine Learning}},
url = {https://www.jair.org/index.php/jair/article/view/12228},
volume = {70},
year = {2021}
}

@article{Roscher2020,
abstract = {Machine learning methods have been remarkably successful for a wide range of
application areas in the extraction of essential information from data. An exciting and
relatively recent development is the uptake of machine learning in the natural sciences,
where the major goal is to obtain novel scientific insights and discoveries from observational
or simulated data. A prerequisite for obtaining a scientific outcome is domain knowledge, which
is needed to gain explainability, but also to enhance scientific consistency. In this article,
we review explainable machine learning in view of applications in the natural sciences and
discuss three core elements that we identified as relevant in this context: transparency,
interpretability, and explainability. With respect to these core elements, we provide a survey
of recent scientific works that incorporate machine learning and the way that explainable
machine learning is used in combination with domain knowledge from the application areas.},
archivePrefix = {arXiv},
arxivId = {1905.08883},
author = {Roscher, Ribana and Bohn, Bastian and Duarte, Marco F. and Garcke, Jochen},
doi = {10.1109/ACCESS.2020.2976199},
eprint = {1905.08883},
issn = {21693536},
journal = {IEEE Access},
keywords = {Explainable machine learning,informed machine learning,interpretability,scientific consistency,transparency},
mendeley-groups = {pudu},
pages = {42200--42216},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{Explainable Machine Learning for Scientific Insights and Discoveries}},
volume = {8},
year = {2020}
}


@article{Belle2021,
abstract = {Artificial intelligence (AI) provides many opportunities to improve private and public life. Discovering patterns and structures in large troves of data in an automated manner is a core component of data science, and currently drives applications in diverse areas such as computational biology, law and finance. However, such a highly positive impact is coupled with a significant challenge: how do we understand the decisions suggested by these systems in order that we can trust them? In this report, we focus specifically on data-driven methods—machine learning (ML) and pattern recognition models in particular—so as to survey and distill the results and observations from the literature. The purpose of this report can be especially appreciated by noting that ML models are increasingly deployed in a wide range of businesses. However, with the increasing prevalence and complexity of methods, business stakeholders in the very least have a growing number of concerns about the drawbacks of models, data-specific biases, and so on. Analogously, data science practitioners are often not aware about approaches emerging from the academic literature or may struggle to appreciate the differences between different methods, so end up using industry standards such as SHAP. Here, we have undertaken a survey to help industry practitioners (but also data scientists more broadly) understand the field of explainable machine learning better and apply the right tools. Our latter sections build a narrative around a putative data scientist, and discuss how she might go about explaining her models by asking the right questions. From an organization viewpoint, after motivating the area broadly, we discuss the main developments, including the principles that allow us to study transparent models vs. opaque models, as well as model-specific or model-agnostic post-hoc explainability approaches. We also briefly reflect on deep learning models, and conclude with a discussion about future research directions.},
archivePrefix = {arXiv},
arxivId = {2009.11698},
author = {Belle, Vaishak and Papantonis, Ioannis},
doi = {10.3389/FDATA.2021.688969},
eprint = {2009.11698},
issn = {2624909X},
journal = {Frontiers in Big Data},
keywords = {black-box models,explainable AI,machine learning,survey,transparent models},
mendeley-groups = {pudu},
month = {jul},
pages = {39},
pmid = {34278297},
publisher = {Frontiers Media S.A.},
title = {{Principles and Practice of Explainable Machine Learning}},
volume = {4},
year = {2021}
}


@article{Lundberg2017,
abstract = {Understanding why a model makes a certain prediction can be as crucial as the
prediction's accuracy in many applications. However, the highest accuracy for large modern
datasets is often achieved by complex models that even experts struggle to interpret, such as
ensemble or deep learning models, creating a tension between accuracy and interpretability. In
response, various methods have recently been proposed to help users interpret the predictions of
complex models, but it is often unclear how these methods are related and when one method is
preferable over another. To address this problem, we present a unified framework for
interpreting predictions, SHAP (SHapley Additive exPlanations). SHAP assigns each feature an
importance value for a particular prediction. Its novel components include: (1) the
identification of a new class of additive feature importance measures, and (2) theoretical
results showing there is a unique solution in this class with a set of desirable properties.
The new class unifies six existing methods, notable because several recent methods in the class
lack the proposed desirable properties. Based on insights from this unification, we present new
methods that show improved computational performance and/or better consistency with human
intuition than previous approaches.},
author = {Lundberg, Scott M and Allen, Paul G and Lee, Su-In},
journal = {Advances in Neural Information Processing Systems},
mendeley-groups = {pudu},
title = {{A Unified Approach to Interpreting Model Predictions}},
url = {https://github.com/slundberg/shap},
volume = {30},
year = {2017}
}

@misc{Selvaraju2017,
abstract = {We propose a technique for producing 'visual explana-tions' for decisions from a
large class of Convolutional Neu-ral Network (CNN)-based models, making them more transparent.
Our approach-Gradient-weighted Class Activation Mapping (Grad-CAM), uses the gradients of any
target concept (say logits for 'dog' or even a caption), flowing into the final convolutional
layer to produce a coarse localiza-tion map highlighting the important regions in the image for
predicting the concept. Unlike previous approaches, Grad-CAM is applicable to a wide variety of
CNN model-families: (1) CNNs with fully-connected layers (e.g. VGG), (2) CNNs used for
structured outputs (e.g. captioning), (3) CNNs used in tasks with multi-modal inputs (e.g.
visual question answering) or reinforcement learning, without architectural changes or
retraining. We combine Grad-CAM with existing fine-grained visualizations to create a high-resolution class-discriminative visualization, Guided Grad-CAM, and apply it to image classification, image captioning, and visual question answering (VQA) models, including ResNet-based architectures. In the context of image classification models , our visualizations (a) lend insights into failure modes of these models (showing that seemingly unreasonable predictions have reasonable explanations), (b) outperform previous methods on the ILSVRC-15 weakly-supervised local-ization task, (c) are more faithful to the underlying model, and (d) help achieve model generalization by identifying dataset bias. For image captioning and VQA, our visual-izations show even non-attention based models can localize inputs. Finally, we design and conduct human studies to measure if Grad-CAM explanations help users establish appropriate trust in predictions from deep networks and show that Grad-CAM helps untrained users successfully discern a 'stronger' deep network from a 'weaker' one even when both make identical predictions. Our code is available at https: //github.com/ramprs/grad-cam/ along with a demo on CloudCV [2] 1 and video at youtu.be/COjUB9Izk6E. * Work done at Virginia Tech. 1},
author = {Selvaraju, Ramprasaath R. and Cogswell, Michael and Das, Abhishek and Vedantam,
Ramakrishna and Parikh, Devi and Batra, Dhruv},
journal = {Proceedings of the IEEE International Conference on Computer Vision},
doi = {10.1109/iccv.2017.74},
mendeley-groups = {pudu},
pages = {618--626},
title = {{Grad-CAM: Visual Explanations From Deep Networks via Gradient-Based Localization}},
url = {http://gradcam.cloudcv.org},
year = {2017}
}


@techreport{Pedregosa2011,
abstract = {Scikit-learn is a Python module integrating a wide range of state-of-the-art
machine learning algorithms for medium-scale supervised and unsupervised problems. This package
focuses on bringing machine learning to non-specialists using a general-purpose high-level
language. Emphasis is put on ease of use, performance, documentation, and API consistency. It
has minimal dependencies and is distributed under the simplified BSD license, encouraging its
use in both academic and commercial settings. Source code, binaries, and documentation can be
downloaded from http://scikit-learn.sourceforge.net.},
author = {Pedregosa, Fabian and Varoquaux, Ga{\"{e}}l and Gramfort, Alexandre and Michel,
Vincent and Thirion, Bertrand and Grisel, Olivier and Blondel, Mathieu and Prettenhofer,
Peter and Weiss, Ron and Dubourg, Vincent and Vanderplas, Jake and Passos, Alexandre and
Cournapeau, David and Brucher, Matthieu and Perrot, Matthieu and Duchesnay, {\'{E}}douard},
booktitle = {Journal of Machine Learning Research},
keywords = {Python,model selection,supervised learning,unsupervised learning},
mendeley-groups = {SuperPaper},
pages = {2825--2830},
title = {{Scikit-learn: Machine Learning in Python}},
url = {http://scikit-learn.sourceforge.net.},
volume = {12},
year = {2011}
}


@article{chollet2018keras,
author = {Chollet, Fran{\c{c}}ois and Others and Chollet, Fran{\c{c}}ois and Others},
journal = {Astrophysics source code library},
keywords = {Software},
pages = {ascl:1806.022},
title = {{Keras: The Python Deep Learning library}},
url = {https://ui.adsabs.harvard.edu/abs/2018ascl.soft06022C/abstract},
year = {2018}
}

@misc{Marholm2022,
author = {Marholm, Sigvald},
doi = {10.5281/ZENODO.6344451},
mendeley-groups = {pudu},
month = {mar},
title = {{sigvaldm/localreg: Multivariate RBF output}},
url = {https://zenodo.org/record/6344451},
year = {2022}
}

@article{Bau2018,
abstract = {We propose a general framework called Network Dissec-tion for quantifying the interpretability of latent representations of CNNs by evaluating the alignment between individual hidden units and a set of semantic concepts. Given any CNN model, the proposed method draws on a broad data set of visual concepts to score the semantics of hidden units at each intermediate convolutional layer. The units with semantics are given labels across a range of objects, parts, scenes, textures, materials, and colors. We use the proposed method to test the hypothesis that interpretability of units is equivalent to random linear combinations of units, then we apply our method to compare the latent representations of various networks when trained to solve different supervised and self-supervised training tasks. We further analyze the effect of training iterations, compare networks trained with different initializations, examine the impact of network depth and width, and measure the effect of dropout and batch normalization on the interpretability of deep visual representations. We demonstrate that the proposed method can shed light on characteristics of CNN models and training methods that go beyond measurements of their discriminative power.},
author = {Bau, David and Zhou, Bolei and Khosla, Aditya and Aude, Oliva and Torralba, Antonio},
title = {{Network Dissection: Quantifying Interpretability of Deep Visual Representation}},
year = {2017},
doi = {10.1109/cvpr.2017.354},
}

@Article{Hunter2007,
  Author    = {Hunter, J. D.},
  Title     = {Matplotlib: A 2D graphics environment},
  Journal   = {Computing in Science \& Engineering},
  Volume    = {9},
  Number    = {3},
  Pages     = {90--95},
  abstract  = {Matplotlib is a 2D graphics package used for Python for
  application development, interactive scripting, and publication-quality
  image generation across user interfaces and operating systems.},
  publisher = {IEEE COMPUTER SOC},
  doi       = {10.1109/MCSE.2007.55},
  year      = 2007
}


@misc{Harris2020,
abstract = {Array programming provides a powerful, compact and expressive syntax for accessing, manipulating and operating on data in vectors, matrices and higher-dimensional arrays. NumPy is the primary array programming library for the Python language. It has an essential role in research analysis pipelines in fields as diverse as physics, chemistry, astronomy, geoscience, biology, psychology, materials science, engineering, finance and economics. For example, in astronomy, NumPy was an important part of the software stack used in the discovery of gravitational waves1 and in the first imaging of a black hole2. Here we review how a few fundamental array concepts lead to a simple and powerful programming paradigm for organizing, exploring and analysing scientific data. NumPy is the foundation upon which the scientific Python ecosystem is constructed. It is so pervasive that several projects, targeting audiences with specialized needs, have developed their own NumPy-like interfaces and array objects. Owing to its central position in the ecosystem, NumPy increasingly acts as an interoperability layer between such array computation libraries and, together with its application programming interface (API), provides a flexible framework to support the next decade of scientific and industrial analysis.},
archivePrefix = {arXiv},
arxivId = {2006.10256},
author = {Harris, Charles R. and Millman, K. Jarrod and van der Walt, St{\'{e}}fan J. and Gommers, Ralf and Virtanen, Pauli and Cournapeau, David and Wieser, Eric and Taylor, Julian and Berg, Sebastian and Smith, Nathaniel J. and Kern, Robert and Picus, Matti and Hoyer, Stephan and van Kerkwijk, Marten H. and Brett, Matthew and Haldane, Allan and del R{\'{i}}o, Jaime Fern{\'{a}}ndez and Wiebe, Mark and Peterson, Pearu and G{\'{e}}rard-Marchant, Pierre and Sheppard, Kevin and Reddy, Tyler and Weckesser, Warren and Abbasi, Hameer and Gohlke, Christoph and Oliphant, Travis E.},
booktitle = {Nature},
doi = {10.1038/s41586-020-2649-2},
eprint = {2006.10256},
file = {:C\:/Users/etgrau/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Harris et al. - 2020 - Array programming with NumPy.pdf:pdf},
issn = {14764687},
keywords = {Computational neuroscience,Computational science,Computer science,Software,Solar physics},
mendeley-groups = {SuperPaper},
month = {sep},
number = {7825},
pages = {357--362},
pmid = {32939066},
publisher = {Nature Research},
title = {{Array programming with NumPy}},
url = {https://doi.org/10.1038/s41586-020-2649-2},
volume = {585},
year = {2020}
}


@article{Kaur2020,
abstract = {Machine learning (ML) models are now routinely deployed in domains ranging from criminal justice to healthcare. With this newfound ubiquity, ML has moved beyond academia and grown into an engineering discipline. To that end, interpretabil-ity tools have been designed to help data scientists and machine learning practitioners better understand how ML models work. However, there has been little evaluation of the extent to which these tools achieve this goal. We study data scien-tists' use of two existing interpretability tools, the InterpretML implementation of GAMs and the SHAP Python package. We conduct a contextual inquiry (N=11) and a survey (N=197) of data scientists to observe how they use interpretability tools to uncover common issues that arise when building and evaluating ML models. Our results indicate that data scientists over-trust and misuse interpretability tools. Furthermore, few of our participants were able to accurately describe the visual-izations output by these tools. We highlight qualitative themes for data scientists' mental models of interpretability tools. We conclude with implications for researchers and tool designers, and contextualize our findings in the social science literature.},
author = {Kaur, Harmanpreet and Nori, Harsha and Jenkins, Samuel and Caruana, Rich and Wallach, Hanna and {Wortman Vaughan}, Jennifer},
doi = {10.1145/3313831.3376219},
isbn = {9781450367080},
keywords = {Author Keywords interpretability,machine learning,user-centric evaluation CCS Concepts •Computing methodologies → Machine learning,•Human-centered computing → User studies},
mendeley-groups = {NN},
title = {{Interpreting Interpretability: Understanding Data Scientists' Use of Interpretability Tools for Machine Learning}},
url = {http://dx.doi.org/10.1145/3313831.3376219}
}


@article{Krishna12022,
abstract = {As various post hoc explanation methods are increasingly being leveraged to
explain complex models in high-stakes settings, it becomes critical to develop
a deeper understanding of if and when the explanations output by these methods
disagree with each other, and how such disagreements are resolved in practice.
However, there is little to no research that provides answers to these critical
questions. In this work, we introduce and study the disagreement problem in
explainable machine learning. More specifically, we formalize the notion of
disagreement between explanations, analyze how often such disagreements occur
in practice, and how do practitioners resolve these disagreements. To this end,
we first conduct interviews with data scientists to understand what constitutes
disagreement between explanations generated by different methods for the same
model prediction, and introduce a novel quantitative framework to formalize
this understanding. We then leverage this framework to carry out a rigorous
empirical analysis with four real-world datasets, six state-of-the-art post hoc
explanation methods, and eight different predictive models, to measure the
extent of disagreement between the explanations generated by various popular
explanation methods. In addition, we carry out an online user study with data
scientists to understand how they resolve the aforementioned disagreements. Our
results indicate that state-of-the-art explanation methods often disagree in
terms of the explanations they output. Our findings also underscore the
importance of developing principled evaluation metrics that enable
practitioners to effectively compare explanations.},
archivePrefix = {arXiv},
arxivId = {2202.01602},
author = {Krishn{\aa}1, Satyapriya and Han˚1, Tessa Han˚1 and Gu, Alex and Pombra, Javin and Jabbari, Shahin and Wu, Zhiwei Steven and Lakkaraju, Himabindu},
eprint = {2202.01602},
isbn = {2202.01602v3},
mendeley-groups = {NN},
month = {feb},
title = {{The Disagreement Problem in Explainable Machine Learning: A Practitioner's Perspective}},
url = {https://arxiv.org/abs/2202.01602v3},
year = {2022}
}


@article{Rojat2021,
abstract = {Most of state of the art methods applied on time series consist of deep learning methods that are too complex to be interpreted. This lack of interpretability is a major drawback, as several applications in the real world are critical tasks, such as the medical field or the autonomous driving field. The explainability of models applied on time series has not gather much attention compared to the computer vision or the natural language processing fields. In this paper, we present an overview of existing explainable AI (XAI) methods applied on time series and illustrate the type of explanations they produce. We also provide a reflection on the impact of these explanation methods to provide confidence and trust in the AI systems.},
archivePrefix = {arXiv},
arxivId = {2104.00950},
author = {Rojat, Thomas and Puget, Rapha{\"{e}}l and Filliat, David and {Del Ser}, Javier and Gelin, Rodolphe and D{\'{i}}az-Rodr{\'{i}}guez, Natalia},
eprint = {2104.00950},
keywords = {Convolutional Neural Networks,Deep learn-ing,Index Terms-Explainable Artificial Intelligence,Recurrent Neural Networks,Time Series},
month = {apr},
title = {{Explainable Artificial Intelligence (XAI) on TimeSeries Data: A Survey}},
url = {https://arxiv.org/abs/2104.00950v1},
year = {2021}
}



@article{Hollig2023,
abstract = {TSInterpret is a python package that enables post-hoc interpretability and explanation of black-box time series classifiers with three lines of code. Due to the specific structure of time series (i.e., non-independent features (Ismail et al., 2020)) and unintuitive visualizations (Siddiqui et al., 2019), traditional interpretability and explainability libraries like Captum (Kokhlikyan et al., 2020), Alibi Explain (Klaise et al., 2021), or tf-explain (Meudec, 2021) find limited usage. TSInterpret specifically addresses the issue of black-box time series classification by providing a unified interface to state-of-the-art interpretation algorithms in combination with default plots. In addition, the package provides a framework for developing additional easy-to-use interpretability methods. Statement of need Temporal data is ubiquitous and encountered in many real-world applications ranging from electronic health records (Rajkomar et al., 2018) to cyber security (Susto et al., 2018). Although deep learning methods have been successful in the field of Computer Vision (CV) and Natural Language Processing (NLP) for almost a decade, application on time series data has only occurred in the past few years (e.g., Fawaz et al., 2019; Rajkomar et al., 2018; Ruiz et al., 2021; Susto et al., 2018). Deep learning models have achieved state-of-the-art results on time series classification (e.g., Fawaz et al., 2019). However, those methods are black boxes due to their complexity which limits their application to high-stake scenarios (e.g., in medicine or autonomous driving), where user trust and understandability of the decision process are crucial. In such scenarios, post-hoc interpretability is useful as it enables the analysis of already trained models without model modification. Much work has been done on post-hoc interpretability in CV and NLP, but most developed approaches are not directly applicable to time series data. The time component impedes the usage of existing methods (Ismail et al., 2020). Thus, increasing effort is put into adapting existing methods to time series (e.g., LEFTIST based on SHAP/Lime (Guillem{\'{e}} et al., 2019), Temporal Saliency Rescaling for Saliency Methods (Ismail et al., 2020), or Counterfactuals (Ates et al., 2021; Delaney et al., 2021; H{\"{o}}llig et al., 2022)). Compared to images or textual data, humans cannot intuitively and instinctively understand the underlying information in time series data. Therefore, time series data, both uni-and multivariate, have an unintuitive nature, lacking an understanding at first sight (Siddiqui et al., 2019). Hence, providing suitable visualizations of time series interpretability becomes crucial. Features Explanations can take various form (see Figure 1). Different use cases or users need different types of explanations. While for a domain expert, counterfactuals are useful, a data scientist or machine learning engineer prefers gradient-based approaches (Ismail et al., 2020) to evaluate H{\"{o}}llig et al. (2023). TSInterpret: A Python Package for the Interpretability of Time Series Classification. Journal of Open Source Software, 8(85), 5220. https://doi.org/10.21105/joss.05220. 1 the model's feature attribution. Figure 1: Explanations. Counterfactual approaches calculate counterexamples by finding a time series close to the original time series that is classified differently, thereby showing decision boundaries. The intuition is to answer the question 'What if?'. TSInterpret implements Ates et al. (2021), a perturbation-based approach for multivariate data, Delaney et al. (2021) for univariate time series, and H{\"{o}}llig et al. (2022) an evolutionary based approach applicable to uni-and multivariate data. Gradient-based approaches (e.g., GradCam) were adapted to time series by Ismail et al. (2020) who proposed rescaling according to time step importance and feature importance. This is applicable to both gradient and perturbation-based methods and based on tf-explain (Meudec, 2021) and captum (Kokhlikyan et al., 2020). LEFTIST by Guillem{\'{e}} et al. (2019) calculates feature importance based on a variety of Lime based on shapelets.},
archivePrefix = {arXiv},
arxivId = {2010.13924},
author = {H{\"{o}}llig, Jacqueline and Kulbach, Cedric and Thoma, Steffen},
doi = {10.21105/JOSS.05220},
eprint = {2010.13924},
isbn = {9780128119686},
issn = {2475-9066},
journal = {Journal of Open Source Software},
month = {may},
number = {85},
pages = {5220},
publisher = {The Open Journal},
title = {{TSInterpret: A Python Package for the Interpretability of Time Series Classification}},
url = {https://joss.theoj.org/papers/10.21105/joss.05220},
volume = {8},
year = {2023}
}


@article{Bhatt2020,
abstract = {Explainable machine learning offers the potential to provide stake-holders with insights into model behavior by using various methods such as feature importance scores, counterfactual explanations, or influential training data. Yet there is little understanding of how organizations use these methods in practice. This study explores how organizations view and use explainability for stakeholder consumption. We find that, currently, the majority of deployments are not for end users affected by the model but rather for machine learning engineers, who use explainability to debug the model itself. There is thus a gap between explainability in practice and the goal of transparency, since explanations primarily serve internal stakeholders rather than external ones. Our study synthesizes the limitations of current explainability techniques that hamper their use for end users. To facilitate end user interaction, we develop a framework for establishing clear goals for explainability. We end by discussing concerns raised regarding explainability. CCS CONCEPTS • Human-centered computing; • Computing methodologies → Philosophical/theoretical foundations of artificial intelligence ; Machine learning; KEYWORDS machine learning, explainability, transparency, deployed systems, qualitative study ACM Reference Format:},
author = {Bhatt, Umang and Xiang, Alice and Sharma, Shubham and Weller, Adrian and Taly, Ankur and Jia, Yunhan and Ghosh, Joydeep and Puri, Ruchir and Moura, José M F and Eckersley, Peter},
doi = {10.1145/3351095.3375624},
isbn = {9781450369367},
keywords = {deployed systems,explainability,machine learning,qualitative study,transparency},
mendeley-groups = {pudu},
title = {{Explainable Machine Learning in Deployment}},
url = {https://doi.org/10.1145/3351095.3375624},
year = {2020}
}




@article{Fonoll-Rubio2022,
author = {Fonoll-Rubio, Robert and Paetel, Stefan and Grau-Luque, Enric and Becerril-Romero, Ignacio and Mayer, Rafael and P{\'{e}}rez-Rodr{\'{i}}guez, Alejandro and Guc, Maxim and Izquierdo-Roca, Victor},
doi = {10.1002/AENM.202103163},
file = {:C\:/Users/Enric/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Fonoll-Rubio et al. - 2022 - Insights into the Effects of RbF-Post-Deposition Treatments on the Absorber Surface of High Efficiency Cu(I.pdf:pdf},
issn = {1614-6840},
journal = {Advanced Energy Materials},
keywords = {2 solar cells,Cu(In,Ga)Se,RbF treatments,combinatorial analysis,machine learning,process monitoring},
mendeley-groups = {SuperPaper},
month = {jan},
pages = {2103163},
publisher = {John Wiley & Sons, Ltd},
title = {{Insights into the Effects of RbF-Post-Deposition Treatments on the Absorber Surface of High Efficiency Cu(In,Ga)Se2 Solar Cells and Development of Analytical and Machine Learning Process Monitoring Methodologies Based on Combinatorial Analysis}},
url = {https://onlinelibrary.wiley.com/doi/full/10.1002/aenm.202103163 https://onlinelibrary.wiley.com/doi/abs/10.1002/aenm.202103163 https://onlinelibrary.wiley.com/doi/10.1002/aenm.202103163},
year = {2022}
}


@article{Grau-Luque2021,
abstract = {Solar cells based on quaternary kesterite compounds like Cu2ZnGeSe4are complex systems where the variation of one parameter can result in changes in the whole system, and, as consequence, in the global performance of the devices. In this way, analyses that take into account this complexity are necessary in order to overcome the existing limitations of this promising Earth-abundant photovoltaic technology. This study presents a combinatorial approach for the analysis of Cu2ZnGeSe4based solar cells. A compositional graded sample containing almost 200 solar cells with different [Zn]/[Ge] compositions is analyzed by means of X-ray fluorescence and Raman spectroscopy, and the results are correlated with the optoelectronic parameters of the different cells. The analysis results in a deep understanding of the stoichiometric limits and point defects formation in the Cu2ZnGeSe4compound, and shows the influence of these parameters on the performance of the devices. Then, intertwined connections between the compositional, vibrational and optoelectronic properties of the cells are revealed using a complex analytical approach. This is further extended using a machine learning algorithm. The latter confirms the correlation between the properties of the Cu2ZnGeSe4compound and the optoelectronic parameters, and also allows proposing a methodology for device performance prediction that is compatible with both research and industrial process monitoring environments. As such, this work not only provides valuable insights for understanding and further developing the Cu2ZnGeSe4photovoltaic technology, but also gives a practical example of the potential of combinatorial analysis and machine learning for the study of complex systems in materials research.},
author = {Grau-Luque, Enric and Anefnaf, Ikram and Benhaddou, Nada and Fonoll-Rubio, Robert and Becerril-Romero, Ignacio and Aazou, Safae and Saucedo, Edgardo and Sekkat, Zouheir and Perez-Rodriguez, Alejandro and Izquierdo-Roca, Victor and Guc, Maxim},
doi = {10.1039/d1ta01299a},
file = {:C\:/Users/Enric/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Grau-Luque et al. - 2021 - Combinatorial and machine learning approaches for the analysis of Cu2ZnGeSe4 influence of the off-stoichiomet.pdf:pdf},
issn = {20507496},
journal = {Journal of Materials Chemistry A},
mendeley-groups = {SuperPaper},
month = {apr},
number = {16},
pages = {10466--10476},
publisher = {Royal Society of Chemistry},
title = {{Combinatorial and machine learning approaches for the analysis of Cu2ZnGeSe4: influence of the off-stoichiometry on defect formation and solar cell performance}},
url = {https://pubs.rsc.org/en/content/articlehtml/2021/ta/d1ta01299a https://pubs.rsc.org/en/content/articlelanding/2021/ta/d1ta01299a},
volume = {9},
year = {2021}
}


@article{VanRossum2009,
abstract = {PYTHON 3 Reference Manual (Python Documentation MANUAL Part 2).Python is an easy to learn object-oriented programming language, which combines power with clear syntax. It has modules, classes, exceptions, very high level data types, and dynamic typing. Python is free software. It can be used with GNU (GNU/Linux), Unix, Microsoft Windows and many other systems.This is a printed softcover copy of the official Python documentation from the latest Python 3.0 distribution. For each copy sold $1 will be donated to the Python Software Foundation by the publisher.This book is part of a brand new six-part series of Python documentation books. Searching for "Python Documentation Manual" will show all six available books.ABOUT THE AUTHOR: Guido van Rossum, is the inventor of Python. Fred L. Drake, Jr. is the official editor of the Python documentation.},
author = {{Van Rossum}, G and Drake, F L},
isbn = {978-1-4414-1269-0},
journal = {Scotts Valley, CA},
mendeley-groups = {SuperPaper},
pages = {242},
title = {{Python 3 Reference Manual; CreateSpace}},
url = {https://www.python.org/},
year = {2009}
}


@article{Caswell2021,
author = {Caswell, Thomas A and Droettboom, Michael and Lee, Antony and de Andrade, Elliott Sales and Hunter, John and Hoffmann, Tim and Firing, Eric and Klymak, Jody and Stansby, David and Varoquaux, Nelle and Nielsen, Jens Hedegaard and Root, Benjamin and May, Ryan and Elson, Phil and Sepp{\"{a}}nen, Jouni K. and Dale, Darren and Lee, Jae-Joon and McDougall, Damon and Straw, Andrew and Hobson, Paul and Gohlke, Christoph and Hannah and Yu, Tony S and Ma, Eric and Vincent, Adrien F. and Silvester, Steven and Moad, Charlie and Kniazev, Nikita and Ernest, Elan and Ivanov, Paul},
doi = {10.5281/ZENODO.4743323},
mendeley-groups = {SuperPaper},
month = {may},
title = {{matplotlib/matplotlib: REL: v3.4.2}},
url = {https://zenodo.org/record/4743323},
year = {2021}
}
